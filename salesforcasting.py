# -*- coding: utf-8 -*-
"""SalesForcasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rihUsWnov9TEJIPYvn6K7Dj0JnaJjIA_

<a href="https://colab.research.google.com/github/ZeeeeCS/Sales-Forcasting-For-retail-store/blob/main/ThirdSalesForcasting.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# **Install needed libraries**
"""

pip install prophet

pip install keras-tuner

"""# **Libraries Import**

**First, we import all the necessary libraries. This includes pandas for data manipulation, numpy for numerical operations, matplotlib and seaborn for static visualizations, plotly for interactive visualizations, sklearn.metrics for evaluating our model, prophet for the time series forecasting model itself, and calendar from pandas to help us identify holidays. We also import warnings to suppress unnecessary output messages and set some default styles for our plots.**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import plotly.io as pio
import tensorflow as tf
import keras_tuner as kt
import matplotlib.dates as mdates
from prophet import Prophet
from pandas.tseries.holiday import USFederalHolidayCalendar as calendar
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import GridSearchCV, ParameterGrid
from sklearn.preprocessing import MinMaxScaler
from hyperopt import fmin, tpe, hp, Trials
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense, Input
from tensorflow.keras.callbacks import EarlyStopping
import warnings
# Ignore warnings
warnings.filterwarnings('ignore')
# consistent styles for matplotlib plots
plt.style.use('ggplot')
plt.style.use('fivethirtyeight')

"""**we define a helper function called mean_absolute_percentage_error (MAPE). MAPE is a common metric used to evaluate the accuracy of forecasts. It measures the average percentage difference between the actual values and the predicted values. We handle potential division by zero by replacing any true zero values with a very small number.**"""

def mean_absolute_percentage_error(y_true, y_pred):
    """Calculates MAPE given y_true and y_pred"""
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    y_true = np.where(y_true == 0, 1e-6, y_true)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

# Uploading API Token
from google.colab import files
files.upload()

"""**Here, we set up the necessary directory and permissions for the Kaggle API to work, downloads the specified dataset ('retail-store-inventory-forecasting-dataset'), and unzips the downloaded file.**"""

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download anirudhchauhan/retail-store-inventory-forecasting-dataset

!unzip retail-store-inventory-forecasting-dataset.zip

# load the Data into DataFrame
data_sales=pd.read_csv('/content/retail_store_inventory.csv')

# shows first 10 rows of the Data
data_sales.head(10)

# a concise summary of the DataFrame
data_sales.info()

# Check the dimensions of the DataFrame
data_sales.shape

# Convert object-type columns to Categorical Columns
data_sales[['Region','Category','Seasonality','Weather Condition']]=data_sales[['Region','Category','Seasonality','Weather Condition']].astype('category')

"""# **Cleaning and Preprocessing**"""

# confirm the data type changes
data_sales.info()

"""**We create a boxplot to visualize how 'Units Sold' are distributed across different 'Regions'.**"""

# Boxplots the distribution of 'Units Sold' by Region
sns.boxplot(data=data_sales,x='Region',y='Units Sold')
plt.show()

# Boxplots the distribution of 'Units Ordered' by Region
sns.boxplot(data=data_sales,x='Region',y='Units Ordered')
plt.show()

# Boxplot for 'Demand Forecast' by product 'Category'
sns.boxplot(data=data_sales,x='Category',y='Demand Forecast')
plt.show()

# Check for Missing Values
data_sales.isnull().sum()

# Descriptive Statistics
data_sales.describe()

"""**to explore the relationships between different numerical variables, we first need to ensure all columns are numeric. We create a copy of the data, drop non-numeric identifiers ('Date', 'Store ID', 'Product ID'), and then convert the categorical columns ('Region', 'Category', etc.) into numerical representations using .cat.codes. Each unique category gets assigned an integer code.**"""

# Correlation Analysis
corrlation=data_sales.copy()
corrlation.drop(columns=['Date','Store ID','Product ID'],inplace=True)

for col in ['Region','Category','Seasonality','Weather Condition']:
    corrlation[col]=corrlation[col].astype('category').cat.codes

corrlation.corr()

"""# **Visualisation**"""

# a heatmap to visualize the correlation matrix
plt.figure(figsize=(10, 6))
sns.heatmap(corrlation.corr(),annot=True,cmap='coolwarm',linewidths=0.5, fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

# Convert 'Date' column to datetime format
data_sales['Date'] = pd.to_datetime(data_sales['Date'])

# Now filter the correct date range
filtered_data = data_sales[(data_sales['Date'] >= '2022-01-01') & (data_sales['Date'] <= '2022-12-31')]

plt.figure(figsize=(12, 6))
sns.lineplot(data=filtered_data, x='Date', y='Demand Forecast')

plt.title('Demand Forecast Over Time (2022)')
plt.xlabel('Date')
plt.ylabel('Demand Forecast')

# line plot showing the 'Demand Forecast' over time for the filtered 2022 data
plt.gca().xaxis.set_major_locator(mdates.MonthLocator())  # Show one label per month
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))  # Format as "Jan 2022"

plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# A histogram helps us understand the distribution of 'Demand Forecast' values to see how frequently different forecast levels occur.
plt.figure(figsize=(12, 6))
sns.histplot(data=data_sales, x='Demand Forecast', bins=30, kde=True)
plt.title('Distribution of Demand Forecast')
plt.xlabel('Demand Forecast')
plt.ylabel('Frequency')
plt.show()

# Extract day of the week from the Date column
data_sales['DayOfWeek'] = pd.to_datetime(data_sales['Date']).dt.day_name()

# Group by day of the week and sum the uDemand Forecast
daily_sales = data_sales.groupby('DayOfWeek')['Demand Forecast'].sum().reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])

# Total demand forecast for each day of the week
plt.figure(figsize=(12, 6))
daily_sales.plot(kind='line', marker='o')
plt.title('Total Demand Forecast by Day of the Week')
plt.xlabel('Day of the Week')
plt.ylabel('Total Demand Forecast')
plt.grid(True)
plt.show()

# Extract the month number (1-12) from the 'Date' column
data_sales['Month'] = pd.to_datetime(data_sales['Date']).dt.month

# line plot showing the total 'Demand Forecast' aggregated by month
plt.figure(figsize=(12, 6))
sns.lineplot(data=data_sales, x='Month', y='Demand Forecast', estimator=sum, ci=None)
plt.title('Total Demand Forecast by Month')
plt.xlabel('Month')
plt.ylabel('Total Demand Forecast')
plt.show()

# distribution of 'Price' values
plt.figure(figsize=(12, 6))
sns.histplot(data=data_sales, x='Price', bins=30, kde=True)
plt.title('Distribution of Price')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

# line plot showing the total 'Demand Forecast' over time
plt.figure(figsize=(12, 6))
sns.lineplot(data=data_sales, x='Date', y='Demand Forecast', hue='Region', estimator=sum, ci=None)
plt.title('Total Demand Forecast by Region Over Time')
plt.xlabel('Date')
plt.ylabel('Total Demand Forecast')
plt.gca().xaxis.set_major_locator(mdates.MonthLocator())  # Show one label per month
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))  # Format as "Jan 2022"
plt.xticks(rotation=45)  # Rotate labels for readability
plt.tight_layout()  # Adjust layout to prevent cutoff
plt.show()

# pie chart showing the proportion of records for each 'Region'
plt.pie(data_sales['Region'].value_counts(), labels=data_sales['Region'].value_counts().index, autopct='%1.1f%%')
plt.title('Distribution of Regions')
plt.show()

# pie chart showing the proportion of records for each product 'Category'
plt.pie(data_sales['Category'].value_counts(), labels=data_sales['Category'].value_counts().index, autopct='%1.1f%%')
plt.title('Distribution of Categories')
plt.show()

# pie chart showing the proportion of records for each Season
plt.pie(data_sales['Seasonality'].value_counts(), labels=data_sales['Seasonality'].value_counts().index, autopct='%1.1f%%')
plt.title('Distribution of Seasonality')
plt.show()

grouped_data_weather = data_sales.groupby(['Category', 'Weather Condition'])['Demand Forecast'].sum().reset_index()

fig=px.bar(grouped_data_weather,x='Category',y='Demand Forecast',color='Weather Condition',color_discrete_map={
        'Rainy': 'blue',
        'Sunny': 'orange',
        'Cloudy': 'grey',
        'Snowy': 'lightblue'
    })
fig.show()

grouped_data_area = data_sales.groupby(['Date', 'Region'])['Demand Forecast'].sum().reset_index()
px.area(grouped_data_area,x='Date',y='Demand Forecast',color='Region')

grouped_data_region = data_sales.groupby(['Category', 'Region'])['Demand Forecast'].sum().reset_index()
px.bar(grouped_data_region,x='Category',y='Demand Forecast',color='Region',barmode='group')

grouped_data_seasonality = data_sales.groupby(['Category', 'Seasonality'])['Demand Forecast'].sum().reset_index()
px.bar(grouped_data_seasonality,x='Category',y='Demand Forecast',color='Seasonality',barmode='group')

"""# **Feature Engineering**

**to understand the time range, Check the minimum and maximum dates in the dataset.**
"""

data_sales['Date'].min(),data_sales['Date'].max()

"""**Prophet requires specific column names: 'ds' for date and 'y' for the value to forecast.**

**We also aggregate the data to the daily level, as Prophet works well with daily data.**
"""

# Prepare Prophet Data by Creating a copy to work with
daily_df = data_sales.copy()
daily_df['Date'] = pd.to_datetime(daily_df['Date'])

# Group by day: total demand per date to get total daily demand
daily_df = daily_df.groupby('Date', as_index=False)['Demand Forecast'].sum()

# Add engineered features
daily_df['day_of_week'] = daily_df['Date'].dt.dayofweek
daily_df['month'] = daily_df['Date'].dt.month
daily_df['is_weekend'] = daily_df['day_of_week'].isin([5, 6]).astype(int)

# Rename columns to 'ds' and 'y' as required by Prophet
daily_df = daily_df.rename(columns={'Date': 'ds', 'Demand Forecast': 'y'})

daily_df.shape

# Define a split date to separate training and testing data
split_date = '2023-10-01'
# Create the training set (data before the split date)
train_df = daily_df[daily_df['ds'] < split_date].copy()
# Create the testing set (data on or after the split date)
test_df = daily_df[daily_df['ds'] >= split_date].copy()

"""**Visualize the train/test split**

Rename columns for clarity in the legend.

Set index to 'ds' for joining and plotting time series.
"""

plot_df = train_df.set_index('ds')[['y']].rename(columns={'y':'Train Demand Forecast'}) \
          .join(test_df.set_index('ds')[['y']].rename(columns={'y':'Test Demand Forecast'}), how='outer')

plot_df.plot(figsize=(15, 7), title='Train/Test Split of Demand Forecast', style=['.', '.']) # Use dots for actual data points
# Add a vertical line at the split date
plt.axvline(pd.to_datetime(split_date), color='r', linestyle='--', label=f'Split Date ({split_date})')
plt.ylabel('Demand Forecast') # Add y-axis label
plt.xlabel('Date') # Add x-axis label
plt.legend()
plt.show()

# Check the shapes (number of rows/days) in the training and testing sets
test_df.shape,train_df.shape

"""# **Modeling and Trends**"""

# Create the Prophet Model with Regressors
m = Prophet()
m.add_regressor('day_of_week')
m.add_regressor('month')
m.add_regressor('is_weekend')

# Train the model
m.fit(train_df[['ds', 'y', 'day_of_week', 'month', 'is_weekend']])

# Create feature dataframe and forecast
future = m.make_future_dataframe(periods=len(test_df), freq='D')
future['day_of_week'] = future['ds'].dt.dayofweek
future['month'] = future['ds'].dt.month
future['is_weekend'] = future['day_of_week'].isin([5, 6]).astype(int)

forecast = m.predict(future)

# Evaluation the Model
forecast_test = forecast[forecast['ds'].isin(test_df['ds'])]
y_true = test_df['y'].values
y_pred = forecast_test['yhat'].values

rmse = np.sqrt(mean_squared_error(y_true, y_pred))
mae = mean_absolute_error(y_true, y_pred)
mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

print(f"RMSE: {rmse:.2f}")
print(f"MAE: {mae:.2f}")
print(f"MAPE: {mape:.2f}%")

"""**Trend and Seasonality Analysis**"""

# Plot Forecast vs Actual
plt.figure(figsize=(12, 6))
plt.plot(test_df['ds'], y_true, label='Actual', color='black')
plt.plot(test_df['ds'], y_pred, label='Forecast', color='blue')
plt.fill_between(test_df['ds'],
                 forecast_test['yhat_lower'],
                 forecast_test['yhat_upper'],
                 color='skyblue', alpha=0.3, label='Confidence Interval')
plt.title('Prophet Forecast vs Actual')
plt.xlabel('Date')
plt.ylabel('Demand Forecast')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# visualize the entire forecast (history + future)
fig, ax = plt.subplots(figsize=(10, 5))
fig = m.plot(forecast, ax=ax)
ax.set_title('Prophet Forecast')
plt.show()

# Shows trend/weekly seasonality, and regressor effects
fig = m.plot_components(forecast)
plt.show()

"""# **Model with Holidays**

**US federal holidays are extracted using the pandas calendar and formatted into a DataFrame compatible with Prophet.**

**These holidays span from the start of the historical data to the end of the forecast period to ensure they are properly considered during both model training and prediction.**

Note: Prophet model needs holidays for both training and prediction periods
"""

# Get US Holidays using pandas calendar
cal = calendar()

# Create a DataFrame of holidays within the range of our daily data
holidays = cal.holidays(start=daily_df.index.min(),
                        end=daily_df.index.max(),
                        return_name=True)

holiday_df = pd.DataFrame(data=holidays,
                          columns=['holiday'])

# Prophet requires columns 'ds' (date) and 'holiday' (name)
holiday_df = holiday_df.reset_index().rename(columns={'index':'ds'})

holiday_df.head()

"""Training the New Model"""

# Commented out IPython magic to ensure Python compatibility.
# # Cell time execution
# %%time
# 
# model_with_holidays = Prophet(holidays=holiday_df)
# # Add a regressors
# model_with_holidays.add_regressor('day_of_week')
# model_with_holidays.add_regressor('month')
# model_with_holidays.add_regressor('is_weekend')
# 
# # Fit the model using the training data
# model_with_holidays.fit(train_df[['ds', 'y', 'day_of_week', 'month', 'is_weekend']])

# predictions on the test set
test_df_for_pred = test_df[['ds', 'day_of_week', 'month', 'is_weekend']]
# Predict using the model trained with holidays
test_fcst_with_hols = model_with_holidays.predict(df=test_df_for_pred)

# Plot components of the forecast
fig = model_with_holidays.plot_components(test_fcst_with_hols)

for ax in fig.get_axes():
    plt.sca(ax)
    plt.xticks(rotation=45)
    plt.tight_layout()

plt.show()

# Plot the actual test data vs the forecast, focusing on the test period (Oct 2023)
fig, ax = plt.subplots(figsize=(10, 5))
# Plot actual values (red dots)
ax.scatter(test_df['ds'], test_df['y'], color='r', label='Actual')

# Plot the forecast from the holiday model using Prophet's plot function
fig = model_with_holidays.plot(test_fcst_with_hols, ax=ax)

# Set plot limits to focus on the test period
lower_bound = pd.to_datetime('2023-10-01')
upper_bound = pd.to_datetime('2023-10-31')

ax.set_xlim(lower_bound, upper_bound)
# Adjust y-limit for better visualization
ax.set_ylim(0, daily_df['y'].max() * 1.1) # Set ylim slightly larger than max observed y
plt.xticks(rotation=45)
ax.set_title('October 2023 Predictions (with Holidays) vs Actual')

# Evaluate the model with holidays on the test set
y_true_test = test_df['y'].values
y_pred_test_hols = test_fcst_with_hols['yhat'].values

print("\nEvaluation Metrics (Model with Holidays):")
print(f"RMSE: {np.sqrt(mean_squared_error(y_true_test, y_pred_test_hols)):.2f}")
print(f"MAE: {mean_absolute_error(y_true_test, y_pred_test_hols):.2f}")
print(f"MAPE: {mean_absolute_percentage_error(y_true_test, y_pred_test_hols):.2f}%")

"""# **Future Forecasting**

"""

# forecast for the next 365 days (1 year)
future_forecast_df = model_with_holidays.make_future_dataframe(periods=365, freq='D')

# Add regressor values for these future dates
future_forecast_df['day_of_week'] = future_forecast_df['ds'].dt.dayofweek
future_forecast_df['month'] = future_forecast_df['ds'].dt.month
future_forecast_df['is_weekend'] = future_forecast_df['day_of_week'].isin([5, 6]).astype(int)

# Make predictions for the future period using the model trained with holidays and regressors
final_forecast = model_with_holidays.predict(future_forecast_df)

# Display the first few rows of the future forecast (date and predicted value)
print("\nSample of Future Forecast (Next 365 days):")
# TAIL of the forecast to see the future dates and predictions
print(final_forecast[['ds','yhat', 'yhat_lower', 'yhat_upper']].tail())

# final forecast including the future period
fig_final = model_with_holidays.plot(final_forecast)
ax_final = fig_final.gca()
ax_final.set_title('Forecast including Future Predictions (with Holidays)')
plt.xlabel("Date")
plt.ylabel("Demand Forecast")
plt.show()

"""# **Hyper Paramaeter Tuning**

**Prophet doesn't fit directly into scikit-learn's GridSearchCV but there are alternatives (manual loops or Prophet's cross_validation)**
"""

# Store Baseline Prophet Results
prophet_baseline_metrics = {
    'Model': 'Prophet (Baseline w/ Holidays & Regressors)',
    'RMSE': 1024.59,
    'MAE': 798.55,
    'MAPE': 5.60
}

print("Baseline Prophet Metrics:")
print(prophet_baseline_metrics)

"""**First we Tune the Tune Prophet changepoint_prior_scale (CPS) to get the best models' (RMSE, MAE, MAPE)**"""

# Define values to try for changepoint_prior_scale
changepoint_scales_to_try = [0.01, 0.05, 0.1, 0.5] # Default is 0.05
results = []

for scale in changepoint_scales_to_try:
    print(f"--- Training with changepoint_prior_scale = {scale} ---")

    # Initialize model with holidays and the specific scale
    m_tuned = Prophet(
        holidays=holiday_df,
        changepoint_prior_scale=scale,
        # Keep other settings like yearly/daily seasonality default for now
    )
    # Add regressors
    m_tuned.add_regressor('day_of_week')
    m_tuned.add_regressor('month')
    m_tuned.add_regressor('is_weekend')

    # Fit the model
    m_tuned.fit(train_df[['ds', 'y', 'day_of_week', 'month', 'is_weekend']])

    # Predict on the test set period
    # Create the future dataframe including test period dates
    # We need the regressors in the future dataframe
    future_tuned = m_tuned.make_future_dataframe(periods=len(test_df), freq='D', include_history=False) # Only need future dates for test eval
    future_tuned = test_df[['ds', 'day_of_week', 'month', 'is_weekend']].merge(future_tuned, on='ds', how='inner')


    # Make predictions
    forecast_tuned = m_tuned.predict(future_tuned[['ds', 'day_of_week', 'month', 'is_weekend']])

    # Evaluate
    y_true_eval = test_df['y'].values
    y_pred_eval = forecast_tuned['yhat'].values

    rmse_eval = np.sqrt(mean_squared_error(y_true_eval, y_pred_eval))
    mae_eval = mean_absolute_error(y_true_eval, y_pred_eval)
    mape_eval = mean_absolute_percentage_error(y_true_eval, y_pred_eval) # Use our function

    # Store results
    results.append({
        'Model': f'Prophet (cps={scale})',
        'RMSE': round(rmse_eval, 2),
        'MAE': round(mae_eval, 2),
        'MAPE': round(mape_eval, 2)
    })

    print(f"Metrics: RMSE={rmse_eval:.2f}, MAE={mae_eval:.2f}, MAPE={mape_eval:.2f}%")
    print("-" * 40)

# Display tuning results
tuning_results_df = pd.DataFrame(results)
print("\nProphet Tuning Results (changepoint_prior_scale):")
print(tuning_results_df)

# Select Best Prophet Model ---
best_prophet_run = tuning_results_df.loc[tuning_results_df['MAE'].idxmin()]

print("\nBest Prophet Run based on MAE:")
print(best_prophet_run)

# Store the best metrics for comparison later
final_prophet_metrics = best_prophet_run.to_dict()

"""**Hyperparameter Tuning using Random Seach CV**"""

# Define the objective function for hyperopt
def objective(params):
    # Extract parameters
    changepoint_prior_scale = params['changepoint_prior_scale']
    seasonality_prior_scale = params['seasonality_prior_scale']
    holidays_prior_scale = params['holidays_prior_scale']

    # Initialize Prophet model with hyperparameters
    model = Prophet(
        changepoint_prior_scale=changepoint_prior_scale,
        seasonality_prior_scale=seasonality_prior_scale,
        holidays_prior_scale=holidays_prior_scale,
        yearly_seasonality=True,
        weekly_seasonality=True,
        daily_seasonality=False
    )

    # Fit the model
    model.add_regressor('day_of_week')
    model.add_regressor('month')
    model.add_regressor('is_weekend')
    model.fit(train_df[['ds', 'y', 'day_of_week', 'month', 'is_weekend']])

    # Make predictions
    forecast = model.predict(future)
    forecast_test = forecast[forecast['ds'].isin(test_df['ds'])]
    y_pred = forecast_test['yhat'].values

    # Calculate RMSE
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))

    # The objective is to minimize RMSE, so we return the RMSE
    return rmse

# Define the search space
space = {
    'changepoint_prior_scale': hp.uniform('changepoint_prior_scale', 0.001, 0.5),
    'seasonality_prior_scale': hp.uniform('seasonality_prior_scale', 0.01, 50),
    'holidays_prior_scale': hp.uniform('holidays_prior_scale', 0.01, 10)
}

# Set up trials to keep track of results
trials = Trials()

# Run Hyperopt to minimize the objective function
best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20, trials=trials)

print(f"Best Hyperparameters: {best}")

# Refit the model with the best hyperparameters
best_model = Prophet(
    changepoint_prior_scale=best['changepoint_prior_scale'],
    seasonality_prior_scale=best['seasonality_prior_scale'],
    holidays_prior_scale=best['holidays_prior_scale'],
    yearly_seasonality=True,
    weekly_seasonality=True,
    daily_seasonality=False
)

best_model.add_regressor('day_of_week')
best_model.add_regressor('month')
best_model.add_regressor('is_weekend')
best_model.fit(train_df[['ds', 'y', 'day_of_week', 'month', 'is_weekend']])

# Forecasting with the best model
forecast = best_model.predict(future)
forecast_test = forecast[forecast['ds'].isin(test_df['ds'])]
y_pred = forecast_test['yhat'].values

# Final evaluation
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
mae = mean_absolute_error(y_true, y_pred)
mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

print(f"Model with Tuned Hyperparameters Metrics:")
print(f"RMSE: {rmse:.2f}")
print(f"MAE: {mae:.2f}")
print(f"MAPE: {mape:.2f}%")

# Store Hyperopt Prophet results for comparison
final_hyperopt_prophet_metrics = {
    'Model': 'Prophet (Hyperopt Tuned)',
    'RMSE': round(rmse, 2),
    'MAE': round(mae, 2),
    'MAPE': round(mape, 2)
}

"""**Residuals**"""

# Residuals: Difference between true values and predicted values
residuals = y_true - y_pred

# Plot residuals
plt.figure(figsize=(10, 6))
plt.plot(forecast_test['ds'], residuals, label='Residuals', color='red')
plt.axhline(0, color='black', linewidth=1)
plt.title('Residuals of Forecast')
plt.xlabel('Date')
plt.ylabel('Residuals')
plt.legend()
plt.show()

# Check the autocorrelation of residuals (if any)
from statsmodels.graphics.tsaplots import plot_acf
plot_acf(residuals)
plt.show()

"""# **LSTM Model**"""

# Define lookback window (how many past steps to use for prediction)
lookback = 30

# Select features: Target ('y') + Regressors used in Prophet
features_to_scale = ['y', 'day_of_week', 'month', 'is_weekend']
target_col_name = 'y' # Explicitly define target column name
target_col_index = features_to_scale.index(target_col_name) # Find index of target

# Keras Tuner Settings
TUNER_DIRECTORY = 'lstm_tuning_dir_final'
TUNER_PROJECT_NAME = 'demand_forecast_lstm_final'
MAX_TRIALS = 15  # How many different hyperparameter sets to try
EXECUTIONS_PER_TRIAL = 1 # How many times to train each set (usually 1 is fine)
TUNER_EPOCHS = 50 # Max epochs during the tuning search phase

# Final Model Training Settings
FINAL_MODEL_EPOCHS = 100 # Max epochs for training the best model found
VALIDATION_SPLIT_RATIO = 0.2 # Use 20% of training data for validation during tuning & final training
EARLY_STOPPING_PATIENCE = 10 # Stop training if validation metric doesn't improve for this many epochs

print("LSTM Configuration Set:")
print(f"Lookback window: {lookback} days")
print(f"Features for LSTM: {features_to_scale}")
print(f"Target column index: {target_col_index}")

"""**Prepare Data Copies and Scale**"""

# --- Prepare Data Specifically for LSTM ---

# Create copies of the relevant parts of train/test DFs to avoid modifying originals
# We use the train_df and test_df created earlier for Prophet
train_lstm_df = train_df[features_to_scale].copy()
test_lstm_df = test_df[features_to_scale].copy()

print(f"Shape of data prepared for LSTM training: {train_lstm_df.shape}")
print(f"Shape of data prepared for LSTM testing: {test_lstm_df.shape}")

# --- Scale the Data ---
# Scaling is crucial for LSTMs. Scale features to the range [0, 1].
# IMPORTANT: Fit the scaler ONLY on the training data to prevent data leakage.
scaler = MinMaxScaler(feature_range=(0, 1))

# Fit on training data and transform it
scaled_train_data = scaler.fit_transform(train_lstm_df)
print(f"\nShape of scaled training data: {scaled_train_data.shape}")

# Use the *same* fitted scaler to transform the test data
scaled_test_data = scaler.transform(test_lstm_df)
print(f"Shape of scaled test data: {scaled_test_data.shape}")

"""**Sequence Creation Function**"""

# --- Define Function to Create Input/Output Sequences ---

# This function converts the time series data into sequences suitable for LSTM input.
# For each sequence, X contains 'lookback' steps of all features,
# and y contains the target value at the step immediately following the sequence.

def create_sequences(data, lookback, target_index):
    """
    Creates sequences of data with a specified lookback window.

    Args:
        data (np.array): Scaled data array (samples, features).
        lookback (int): Number of previous time steps to use as input features.
        target_index (int): Index of the target variable column in the data array.

    Returns:
        tuple: A tuple containing:
            - np.array: Input sequences (X) with shape (samples, lookback, features).
            - np.array: Target values (y) with shape (samples,).
    """
    X, y = [], []
    for i in range(len(data) - lookback):
        # Input sequence: Features from index i to i+lookback-1
        X.append(data[i:(i + lookback), :])
        # Target value: The target feature at index i+lookback
        y.append(data[i + lookback, target_index])
    return np.array(X), np.array(y)

print("`create_sequences` function defined.")

"""**Generate Training and Testing Sequences**"""

# --- Create Training Sequences ---
X_train_seq, y_train_seq = create_sequences(scaled_train_data, lookback, target_col_index)
print("Shape of Training Sequences (X):", X_train_seq.shape) # Expected: (samples, lookback, num_features)
print("Shape of Training Targets (y):", y_train_seq.shape)

# --- Create Testing Sequences ---
# To create the first sequence for the test set, we need 'lookback' data points
# preceding the test set. We take these from the end of the training data.
combined_data_for_test_seq = np.concatenate((scaled_train_data[-lookback:], scaled_test_data), axis=0)

X_test_seq, y_test_seq = create_sequences(combined_data_for_test_seq, lookback, target_col_index)
print("\nShape of Testing Sequences (X):", X_test_seq.shape) # Expected: (samples, lookback, num_features)
print("Shape of Testing Targets (y):", y_test_seq.shape) # Corresponds to scaled test 'y' values

# --- Verification ---
num_features = len(features_to_scale)
expected_train_samples = len(scaled_train_data) - lookback
expected_test_samples = len(scaled_test_data) # Each test point should have a corresponding sequence/target

assert X_train_seq.shape[1:] == (lookback, num_features), "Train X dimensions incorrect!"
assert X_test_seq.shape[1:] == (lookback, num_features), "Test X dimensions incorrect!"
assert len(X_train_seq) == len(y_train_seq), "Train X and y length mismatch!"
assert len(X_test_seq) == len(y_test_seq), "Test X and y length mismatch!"
assert len(y_test_seq) == expected_test_samples, "Test y length mismatch with original test set size!"
print("\nSequence shapes and lengths verified.")

"""**Define Tunable LSTM Model Function**"""

# --- Define LSTM Model Building Function for Keras Tuner ---
# This function defines the structure of the LSTM model and the hyperparameters
# that Keras Tuner will optimize (e.g., number of units, dropout rate, optimizer, learning rate, architecture type).

from tensorflow.keras.layers import Bidirectional # Make sure Bidirectional is imported

# Define input shape based on created sequences: (timesteps, features)
input_shape_lstm = (lookback, X_train_seq.shape[2]) # e.g., (30, 4)

def build_tuned_lstm_model(hp):
    """Builds a tunable LSTM model (Simple, Stacked, or Bidirectional)."""
    model = Sequential(name="LSTM_TimeSeries_Model")
    model.add(Input(shape=input_shape_lstm, name="Input_Layer"))

    # --- Tune Architecture Type ---
    model_type = hp.Choice('model_type', ['simple', 'stacked', 'bidirectional'])

    # --- Tune Common Hyperparameters ---
    hp_units_1 = hp.Int('units_1', min_value=32, max_value=128, step=32)
    hp_dropout_1 = hp.Float('dropout_1', min_value=0.1, max_value=0.4, step=0.1)

    # --- Build Chosen Architecture ---
    if model_type == 'simple':
        model.add(LSTM(units=hp_units_1, activation='tanh', return_sequences=False, name="LSTM_Layer"))
        model.add(Dropout(rate=hp_dropout_1, name="Dropout_Layer"))

    elif model_type == 'stacked':
        # First LSTM layer (must return sequences for the next layer)
        model.add(LSTM(units=hp_units_1, activation='tanh', return_sequences=True, name="LSTM_Layer_1"))
        model.add(Dropout(rate=hp_dropout_1, name="Dropout_Layer_1"))
        # Second LSTM layer (tune units independently)
        hp_units_2 = hp.Int('units_2', min_value=16, max_value=64, step=16)
        hp_dropout_2 = hp.Float('dropout_2', min_value=0.1, max_value=0.4, step=0.1)
        model.add(LSTM(units=hp_units_2, activation='tanh', return_sequences=False, name="LSTM_Layer_2"))
        model.add(Dropout(rate=hp_dropout_2, name="Dropout_Layer_2"))

    elif model_type == 'bidirectional':
        model.add(Bidirectional(LSTM(units=hp_units_1, activation='tanh', return_sequences=False, name="BiLSTM_Layer")))
        model.add(Dropout(rate=hp_dropout_1, name="Dropout_Layer_Bi"))

    # --- Output Layer ---
    model.add(Dense(1, name="Output_Layer")) # Single output node for regression

    # --- Tune Optimizer and Learning Rate ---
    optimizer_choice = hp.Choice('optimizer', values=['adam', 'rmsprop'])
    learning_rate = hp.Float("lr", min_value=1e-4, max_value=1e-2, sampling="log")

    if optimizer_choice == 'adam':
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    else: # optimizer_choice == 'rmsprop'
        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)

    # --- Compile Model ---
    model.compile(
        optimizer=optimizer,
        loss='mean_squared_error', # Suitable loss function for regression
        metrics=['mean_absolute_error'] # Track MAE during training/tuning
    )
    return model

print(f"LSTM model building function `build_tuned_lstm_model` defined for Keras Tuner.")
print(f"Expected input shape for LSTM: {input_shape_lstm}")

"""**Initialize Keras Tuner**"""

# --- Initialize Keras Tuner ---
# We use RandomSearch to explore the hyperparameter space defined in the build function.
# The objective is to minimize the validation MAE ('val_mean_absolute_error').

tuner = kt.RandomSearch(
    hypermodel=build_tuned_lstm_model, # The function that defines the model
    objective=kt.Objective("val_mean_absolute_error", direction="min"), # Goal: minimize validation MAE
    max_trials=MAX_TRIALS, # Number of different hyperparameter combinations to test
    executions_per_trial=EXECUTIONS_PER_TRIAL, # How many times to train each combination
    directory=TUNER_DIRECTORY, # Folder to save results
    project_name=TUNER_PROJECT_NAME,
    overwrite=True # Start fresh, delete previous tuner results in this directory
)

# Display a summary of the hyperparameters being tuned
tuner.search_space_summary()

"""**Run Keras Tuner Search**"""

# --- Perform Hyperparameter Search ---
# The tuner trains different model versions on the training sequences,
# using a portion of it (validation_split) to evaluate performance and find the best HPs.
# EarlyStopping helps prevent overfitting and speeds up the search.

print("\n--- Starting Keras Tuner Hyperparameter Search ---")

# Define Early Stopping callback for the tuning process
stop_early_tuner = EarlyStopping(
    monitor='val_mean_absolute_error', # Metric to watch
    patience=EARLY_STOPPING_PATIENCE,  # Number of epochs with no improvement before stopping
    restore_best_weights=True # Restore model weights from the epoch with the best validation MAE
)

# Start the search process
# IMPORTANT: Do NOT use the test sequences (X_test_seq, y_test_seq) during tuning!
tuner.search(
    X_train_seq, y_train_seq,
    epochs=TUNER_EPOCHS, # Maximum epochs per trial
    validation_split=VALIDATION_SPLIT_RATIO, # Use part of training data for validation
    callbacks=[stop_early_tuner],
    verbose=1 # Show progress for each trial
)

print("\n--- Keras Tuner Search Complete ---")

# Display a summary of the top trial results
tuner.results_summary(num_trials=5) # Show top 5 best trials

"""**Get Best Hyperparameters & Build Final Model**"""

# --- Retrieve Best Hyperparameters and Build the Final Model ---

# Get the hyperparameters object from the best trial found by the tuner
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0] # Get the single best set of HPs

print("\n✅ Best Hyperparameters Found by Tuner:")
print(f"- Model Type: {best_hps.get('model_type')}")
print(f"- Optimizer: {best_hps.get('optimizer')}")
print(f"- Learning Rate: {best_hps.get('lr'):.5f}") # Format learning rate

# Print units and dropout based on the best model type found
best_model_type = best_hps.get('model_type')
print(f"- Units 1: {best_hps.get('units_1')}")
print(f"- Dropout 1: {best_hps.get('dropout_1'):.2f}")
if best_model_type == 'stacked':
    # These HPs only exist if 'stacked' was the best model type
    print(f"- Units 2: {best_hps.get('units_2')}")
    print(f"- Dropout 2: {best_hps.get('dropout_2'):.2f}")

# Build the final model using the best hyperparameters identified
print("\nBuilding final LSTM model with best hyperparameters...")
best_lstm_model = tuner.hypermodel.build(best_hps)

# Display the architecture of the final, best model
best_lstm_model.summary()

"""** Train Final LSTM Model**"""

# --- Train the Final LSTM Model ---
# Now, train the best model configuration on the *entire* training sequence dataset.
# We still use a validation split to monitor for overfitting during this final training phase.

print("\n--- Training Final LSTM Model on Full Training Data ---")

# Re-initialize Early Stopping for final training (often monitor 'val_loss')
stop_early_final = EarlyStopping(
    monitor='val_loss', # Monitor validation loss (MSE)
    patience=EARLY_STOPPING_PATIENCE,
    restore_best_weights=True # Restore weights from the best epoch based on val_loss
)

# Train the model
history = best_lstm_model.fit(
    X_train_seq, y_train_seq,
    epochs=FINAL_MODEL_EPOCHS, # Train for potentially more epochs than during tuning
    validation_split=VALIDATION_SPLIT_RATIO, # Monitor performance on validation set
    callbacks=[stop_early_final],
    verbose=1 # Show training progress
)

print("\n--- Final LSTM Model Training Complete ---")

# --- Evaluate Final LSTM Model Performance on Unseen Test Data ---
# Make predictions on the test sequences and inverse transform to the original scale.
# Calculate standard regression metrics (RMSE, MAE, MAPE).

print("\n--- Evaluating Final LSTM Model on Test Set ---")

# 1. Make predictions (output will be scaled) using the overall best model
predictions_scaled = best_lstm_model.predict(X_test_seq)

# 2. Inverse Transform Predictions and Actuals
num_features_scaled = scaled_train_data.shape[1]
dummy_pred = np.zeros((len(predictions_scaled), num_features_scaled))
dummy_true = np.zeros((len(y_test_seq), num_features_scaled))
dummy_pred[:, target_col_index] = predictions_scaled.ravel()
dummy_true[:, target_col_index] = y_test_seq.ravel()
predictions_original_scale = scaler.inverse_transform(dummy_pred)[:, target_col_index]
y_test_original_scale = scaler.inverse_transform(dummy_true)[:, target_col_index]

# 3. Calculate Evaluation Metrics for the overall best model
lstm_rmse = np.sqrt(mean_squared_error(y_test_original_scale, predictions_original_scale))
lstm_mae = mean_absolute_error(y_test_original_scale, predictions_original_scale)
lstm_mape = mean_absolute_percentage_error(y_test_original_scale, predictions_original_scale)

print(f"\nOverall Best LSTM ({best_model_type.capitalize()}) Test Set Performance (Original Scale):") # Clarify this is the overall best
print(f"RMSE: {lstm_rmse:.2f}")
print(f"MAE:  {lstm_mae:.2f}")
print(f"MAPE: {lstm_mape:.2f}%")

# Store LSTM results for the final comparison table
final_lstm_metrics = {
    'Model': f'LSTM ({best_model_type.capitalize()}, Tuned - Overall Best)', # Be specific
    'RMSE': round(lstm_rmse, 2),
    'MAE': round(lstm_mae, 2),
    'MAPE': round(lstm_mape, 2)
}
print("\nStored metrics for the overall best LSTM model.")

"""Build and Train a Specific Bidirectional Model"""

# --- Build and Train a Specific Bidirectional Model for Comparison ---
print("\n--- Building and Training a Specific Bidirectional LSTM ---")

# Use HPs from the overall best run as a starting point, but force model_type
forced_bi_hps = best_hps.copy() # Start with best HPs

# Manually construct the BiLSTM model structure
forced_bi_model = Sequential(name="Forced_BiLSTM_Model")
forced_bi_model.add(Input(shape=input_shape_lstm, name="Input_Layer"))
forced_bi_model.add(Bidirectional(LSTM(units=forced_bi_hps.get('units_1'), # Use tuned units_1
                                        activation='tanh',
                                        return_sequences=False, # Usually False for last layer before Dense
                                        name="BiLSTM_Layer")))
# Use dropout_1, as BiLSTM in the tuner function only had one dropout layer specified this way
forced_bi_model.add(Dropout(rate=forced_bi_hps.get('dropout_1'), name="Dropout_Layer_Bi"))
forced_bi_model.add(Dense(1, name="Output_Layer"))

# Compile using tuned optimizer and LR from the best overall run
if forced_bi_hps.get('optimizer') == 'adam':
    optimizer_forced = tf.keras.optimizers.Adam(learning_rate=forced_bi_hps.get('lr'))
else:
    optimizer_forced = tf.keras.optimizers.RMSprop(learning_rate=forced_bi_hps.get('lr'))
forced_bi_model.compile(optimizer=optimizer_forced, loss='mean_squared_error', metrics=['mean_absolute_error'])

print("\nForced Bidirectional Model Architecture:")
forced_bi_model.summary()

# Train this specific model
print("\nTraining the forced Bidirectional model...")
stop_early_forced = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)
history_forced_bi = forced_bi_model.fit(
    X_train_seq, y_train_seq,
    epochs=FINAL_MODEL_EPOCHS, # Use same epoch count for fairness
    validation_split=VALIDATION_SPLIT_RATIO,
    callbacks=[stop_early_forced],
    verbose=0 # Set verbose=0 to keep output clean, or 1 to see progress
)
print("--- Forced Bidirectional Model Training Complete ---")

# --- Evaluate the Forced Bidirectional Model ---
print("\n--- Evaluating Forced Bidirectional LSTM on Test Set ---")

# 1. Make predictions using the specifically trained BiLSTM model
predictions_scaled_forced_bi = forced_bi_model.predict(X_test_seq)

# 2. Inverse transform predictions (actuals are the same: y_test_original_scale)
dummy_pred_forced_bi = np.zeros((len(predictions_scaled_forced_bi), num_features_scaled))
dummy_pred_forced_bi[:, target_col_index] = predictions_scaled_forced_bi.ravel()
predictions_original_forced_bi = scaler.inverse_transform(dummy_pred_forced_bi)[:, target_col_index]

# y_test_original_scale was calculated in Cell 10 and remains the same ground truth

# 3. Calculate metrics for this specific BiLSTM model
bi_lstm_rmse = np.sqrt(mean_squared_error(y_test_original_scale, predictions_original_forced_bi))
bi_lstm_mae = mean_absolute_error(y_test_original_scale, predictions_original_forced_bi)
bi_lstm_mape = mean_absolute_percentage_error(y_test_original_scale, predictions_original_forced_bi)

print("\nForced Bidirectional LSTM Test Set Performance (Original Scale):")
print(f"RMSE: {bi_lstm_rmse:.2f}")
print(f"MAE:  {bi_lstm_mae:.2f}")
print(f"MAPE: {bi_lstm_mape:.2f}%")

# Store metrics for comparison table
final_bi_lstm_metrics = {
    'Model': 'LSTM (Bidirectional, Specific Eval)', # Clear label
    'RMSE': round(bi_lstm_rmse, 2),
    'MAE': round(bi_lstm_mae, 2),
    'MAPE': round(bi_lstm_mape, 2)
}
print("\nStored metrics for the specifically evaluated Bidirectional LSTM model.")

"""Plot Training History - Plots the Overall Best Model's History"""

# --- Plot LSTM Training & Validation Loss Curves ---
# Visualizing the loss helps understand if the model trained well and if overfitting occurred.

print("\n--- Plotting LSTM Training History ---")
plt.figure(figsize=(10, 6))

# Plot training and validation loss (MSE)
plt.plot(history.history['loss'], label='Training Loss (MSE)')
plt.plot(history.history['val_loss'], label='Validation Loss (MSE)')

# Optionally plot MAE if tracked
if 'mean_absolute_error' in history.history:
    plt.plot(history.history['mean_absolute_error'], label='Training MAE', linestyle=':')
    plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE', linestyle=':')

plt.title(f'LSTM Final Model Training History ({best_model_type.capitalize()})')
plt.xlabel('Epoch')
plt.ylabel('Error Metric Value')
plt.legend()
plt.grid(True)
plt.ylim(bottom=0) # Loss/Error metrics shouldn't be negative
plt.show()

# --- Plot LSTM Predictions vs Actual Values for the Test Set ---
# This visual comparison shows how well the model's forecast matches the real data.

print("\n--- Plotting LSTM Predictions vs Actuals (Test Set) ---")

# Get the dates corresponding to the test set predictions/actuals
# y_test_original_scale corresponds directly to the test_df 'y' values after the lookback period adjustment
# Ensure test_dates align with the predictions. Since X_test_seq/y_test_seq represent predictions
# *after* the sequence, they align directly with the dates in test_df.
test_dates = test_df['ds'].reset_index(drop=True)

# Ensure lengths match before plotting
if len(test_dates) == len(y_test_original_scale) == len(predictions_original_scale):
    plt.figure(figsize=(15, 7))
    plt.plot(test_dates, y_test_original_scale, label='Actual Demand', color='black', marker='.', linestyle='None', alpha=0.7)
    plt.plot(test_dates, predictions_original_scale, label=f'LSTM ({best_model_type.capitalize()}) Forecast', color='red', linestyle='-', alpha=0.9)
    plt.title('LSTM Forecast vs Actual Demand (Test Set)')
    plt.xlabel('Date')
    plt.ylabel('Demand Forecast (Original Scale)')
    plt.legend()
    plt.grid(True)
    # Format the x-axis for dates
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator()) # Auto-adjust date ticks
    plt.gcf().autofmt_xdate() # Rotate date labels automatically
    plt.tight_layout()
    plt.show()
else:
    print("Error: Length mismatch prevents plotting.")
    print(f"Test Dates: {len(test_dates)}, Actuals: {len(y_test_original_scale)}, Predictions: {len(predictions_original_scale)}")

"""**Prophet and LSTM Models Comparison**"""

# Ensure the 'final_prophet_metrics' dictionary from the Prophet section exists

# Ensure the metrics for the specific BiLSTM evaluation exist

# Ensure the metrics for the hyperopt Prophet model exist
#if 'final_hyperopt_prophet_metrics' not in locals() and 'final_hyperopt_prophet_metrics' not in globals():
#     print("Warning: 'final_hyperopt_prophet_metrics' not found. Using placeholder.")
#     final_hyperopt_prophet_metrics = {'Model': 'Prophet (Hyperopt Tuned)', 'RMSE': 0.0, 'MAE': 0.0, 'MAPE': 0.0} # Placeholder

# Create a list of all metrics dictionaries (including the hyperopt one)
all_metrics = [
    final_prophet_metrics,          # Best from manual CPS tuning
    final_hyperopt_prophet_metrics, # Best from hyperopt tuning
    final_bi_lstm_metrics,          # Specifically evaluated BiLSTM
    final_lstm_metrics,             # Best overall LSTM
]

# Create a DataFrame for comparison
comparison_df = pd.DataFrame(all_metrics)

# Display the comparison table, using the 'Model' column as the index
print(comparison_df.set_index('Model'))



"""GIT and MLFLOW"""

!git init
!git add .
!git commit -m "Initial commit of project code"

pip install mlflow dvc dvc[gdrive]

!dvc init

!git config --global user.email "mohamed.abdelrazek@ejust.edu.eg"
!git config --global user.name "Mohamed"

!git add .dvc .gitignore
!git commit -m "Initialize DVC"

!dvc remote list

!dvc remote default

!dvc remote add -d --force gdrive gdrive://1G5LV4DqHw0tDP6EehmX8kFU7ZurQbEtu

!git add .dvc/config
!git commit -m "Configure DVC remote storage (Google Drive)"

!dvc remote modify gdrive default true
!git add .dvc/config
!git commit -m "Set gdrive as default DVC remote"

# Create a folder in your Google Drive for DVC storage (e.g., "SalesForecastDVC")
# Get the folder ID from the URL (the long string after /folders/)
!dvc remote add -d gdrive gdrive://1G5LV4DqHw0tDP6EehmX8kFU7ZurQbEtu
!git add .dvc/config # Add the updated config file
!git commit -m "Configure DVC remote storage (Google Drive)"
# The first time you push/pull, DVC will guide you through authentication.

# Create a directory outside your project folder for storage
# Example: /path/to/your/dvc_storage
!dvc remote add -d local /path/to/your/dvc_storage
!git add .dvc/config
!git commit -m "Configure DVC remote storage (local)"

!mkdir data
!mv retail_store_inventory.csv data/
!git add data/ # Tell git to track the directory, even if DVC ignores the content later
!git commit -m "Organize data into data/ directory"

!dvc add data/retail_store_inventory.csv

!git add data/retail_store_inventory.csv.dvc .gitignore
!git commit -m "Track raw dataset with DVC"

!dvc push

import mlflow
import mlflow.prophet
import mlflow.tensorflow
from sklearn.metrics import mean_squared_error, mean_absolute_error # Ensure these are imported
# Your MAPE function definition should be available

# --- Example: Wrapping Prophet CPS Tuning Loop ---
changepoint_scales_to_try = [0.01, 0.05, 0.1, 0.5]
results = [] # Keep this for local results table if desired

for scale in changepoint_scales_to_try:
    # Start an MLflow run for each scale
    with mlflow.start_run(run_name=f"Prophet_CPS_{scale}"): # Give runs meaningful names
        print(f"--- Training with changepoint_prior_scale = {scale} ---")

        # Log the parameter being tuned
        mlflow.log_param("changepoint_prior_scale", scale)
        mlflow.log_param("model_type", "Prophet") # Log model type

        # --- Your existing model initialization and training code ---
        m_tuned = Prophet(
            holidays=holiday_df,
            changepoint_prior_scale=scale,
        )
        m_tuned.add_regressor('day_of_week')
        m_tuned.add_regressor('month')
        m_tuned.add_regressor('is_weekend')
        m_tuned.fit(train_df[['ds', 'y', 'day_of_week', 'month', 'is_weekend']])
        # --- End of existing training code ---

        # --- Your existing prediction code ---
        future_tuned = m_tuned.make_future_dataframe(periods=len(test_df), freq='D', include_history=False)
        future_tuned = test_df[['ds', 'day_of_week', 'month', 'is_weekend']].merge(future_tuned, on='ds', how='inner')
        forecast_tuned = m_tuned.predict(future_tuned[['ds', 'day_of_week', 'month', 'is_weekend']])
        # --- End of existing prediction code ---

        # --- Your existing evaluation code ---
        y_true_eval = test_df['y'].values
        y_pred_eval = forecast_tuned['yhat'].values
        rmse_eval = np.sqrt(mean_squared_error(y_true_eval, y_pred_eval))
        mae_eval = mean_absolute_error(y_true_eval, y_pred_eval)
        mape_eval = mean_absolute_percentage_error(y_true_eval, y_pred_eval)
        # --- End of existing evaluation code ---

        # Log metrics to MLflow
        mlflow.log_metric("rmse", rmse_eval)
        mlflow.log_metric("mae", mae_eval)
        mlflow.log_metric("mape", mape_eval)

        print(f"Metrics: RMSE={rmse_eval:.2f}, MAE={mae_eval:.2f}, MAPE={mape_eval:.2f}%")

        # Log the Prophet model using the MLflow flavor
        # This saves the model in a standard format and logs it as an artifact
        mlflow.prophet.log_model(m_tuned, artifact_path="prophet-model")

        # (Optional) Log plots or other artifacts
        # fig = m_tuned.plot(forecast_tuned)
        # plt.savefig("forecast_plot.png")
        # mlflow.log_artifact("forecast_plot.png")
        # plt.close(fig) # Close plot to free memory

        print("-" * 40)

    # Append results to local list (optional)
    results.append({
        'Model': f'Prophet (cps={scale})', 'RMSE': round(rmse_eval, 2),
        'MAE': round(mae_eval, 2), 'MAPE': round(mape_eval, 2)
    })

# --- Similarly wrap your hyperopt section ---
# In the objective function or after finding the best model:
# with mlflow.start_run(run_name="Prophet_Hyperopt_Best"):
#    mlflow.log_params(best) # Log best hyperopt params
#    # ... retrain best model ...
#    mlflow.log_metric("rmse", final_rmse)
#    mlflow.log_metric("mae", final_mae)
#    mlflow.log_metric("mape", final_mape)
#    mlflow.prophet.log_model(best_model, artifact_path="prophet-hyperopt-model")


# --- Example: Wrapping Final LSTM Training ---
# After finding best HPs with Keras Tuner
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
best_lstm_model = tuner.hypermodel.build(best_hps)

with mlflow.start_run(run_name="LSTM_Final_Training"):
    # Log best hyperparameters found by tuner
    mlflow.log_params(best_hps.values) # Log all HPs from the best trial
    mlflow.log_param("lookback_window", lookback) # Log other relevant params

    # --- Your existing final LSTM training code ---
    history = best_lstm_model.fit(...)
    # --- End of existing training code ---

    # --- Your existing LSTM evaluation code ---
    predictions_scaled = best_lstm_model.predict(X_test_seq)
    # ... inverse scaling ...
    lstm_rmse = np.sqrt(mean_squared_error(y_test_original_scale, predictions_original_scale))
    lstm_mae = mean_absolute_error(y_test_original_scale, predictions_original_scale)
    lstm_mape = mean_absolute_percentage_error(y_test_original_scale, predictions_original_scale)
    # --- End of existing evaluation code ---

    # Log final test metrics
    mlflow.log_metric("final_test_rmse", lstm_rmse)
    mlflow.log_metric("final_test_mae", lstm_mae)
    mlflow.log_metric("final_test_mape", lstm_mape)

    # Log the TensorFlow/Keras model
    mlflow.tensorflow.log_model(best_lstm_model, artifact_path="lstm-model")

    # (Optional) Log training history plot
    # plt.figure()
    # plt.plot(...)
    # plt.savefig("lstm_history.png")
    # mlflow.log_artifact("lstm_history.png")
    # plt.close()

# --- Do similar logging for the specific BiLSTM evaluation run ---

mlflow ui

# Example for LSTM
best_lstm_model.save("models/best_lstm_model.h5")

dvc add models/best_lstm_model.h5

git add models/best_lstm_model.h5.dvc models/.gitignore # DVC might update .gitignore for models/
git commit -m "Track trained LSTM model v1 with DVC"

dvc push

git add your_notebook.ipynb # Or your .py script
git commit -m "Integrate MLflow tracking for experiments"

